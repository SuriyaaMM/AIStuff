# Transformer 

- **Implementation is done using `jax`**

- multiheadAttention.py Simple Scaled Dot Product attention mechanism implemented for multiple heads

- encoder.py Simple encoder (without embeddings) class for an typical transformer in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762)